<!-- mpc/index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Temperature Control Signal Forcast - Attention</title>
  <style>
    body {
      font-family: sans-serif;
      padding: 20px;
      max-width: 900px;
      margin: auto;
      line-height: 1.6;
    }
    img {
      width: 100%;
      height: auto;
      border-radius: 6px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
      display: block;
      margin-left: auto;
      margin-right: auto;
    }
    figure {
      margin: 30px 0;
    }
    figcaption {
      text-align: center;
      font-size: 0.9em;
      color: #555;
    }
    h1 {
      font-size: 2em;
      font-weight: 700;
      margin-bottom: 1em;
      color: #111;
    }
    h2 {
      font-size: 1.8em;
      font-weight: 600;
      margin-top: 2em;
      color: #222;
    }
    h3 {
      font-size: 1.5em;
      font-weight: 600;
      margin-top: 1.5em;
      color: #333;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <h1>Temperature Control Using Seq2Seq and Attention</h1>

  <h2>Project Overview</h2>
  <p>
    Closed-loop feedback controllers are commonly used for temperature regulation in the field of process control. Designed for stability and reliability, feedback controllers maintain process stability with minimal deviation from target values. However, feedback regulators face limitations in complex scenarios, as they rely solely on real-time data without learning from past historical information.
  </p>
  <p>
    Recent research on Sequence-to-Sequence machine learning models with attention mechanisms offers promising solutions for more adaptive, dynamic temperature control. The work presented in this presentation of a thesis project, develops a tailored machine learning model based on scaled dot-product attention to predict temperature control signals from error values. The tailored model uses simulated temperature control error and predicts control signal. The performance of the attention-based model was compared to a baseline sequence-to-sequence model, demonstrating improved alignment with the target control signal and highlighting its potential for effective temperature regulation.
  </p>

  <h2>Background: Closed-Loop Control Systems</h2>
  <p>
    In traditional closed-loop control systems (CLCS), a controller adjusts the input to a system based on the difference between the desired output (the set-point) and the actual output. This is often done using a proportional-integral (PI) or proportional-integral-derivative (PID) controller.
  </p>
  <p>
    PI control combines immediate error correction with a memory of past errors. While simple and effective in many industrial settings, these controllers can struggle when set-points change rapidly or when system dynamics are nonlinear or poorly understood. Derivative terms, often used in PID control, are sensitive to noise and are excluded in this project.
  </p>

  <figure>
    <img src="Temp-SP_Vs_Control-Signal.PNG" alt="Set-point vs Control Signal" />
    <figcaption>Figure: Simulated response of a traditional PI-controlled system showing how the control signal responds to temperature changes.</figcaption>
  </figure>

  <h2>Models Overview</h2>
  <p>
    Two models were developed for comparison:
  </p>
  <ul>
    <li><strong>Baseline Model:</strong> A basic Seq2Seq model using gated recurrent units (GRUs).</li>
    <li><strong>Attention Model:</strong> An enhanced version with scaled dot-product attention between encoder and decoder also using GRUS.</li>
  </ul>

  <p>
    Both models take an input sequence of past errors and output a predicted sequence of control signals. The attention model is designed to selectively focus on the most relevant parts of the input sequence (error) when making predictions.
  </p>

  <h2>Attention-Enhanced Model</h2>
  <p>
    Sequence-to-sequence (Seq2Seq) models are comprised of networks of GRUs commonly used in natural language processing. An application for these seq2seq models in NLP enables the prediction of the next word in a sentence over long sentences. However, longer and longer sequences bring about problems of gradient stability, which is mitigated by techniques such as attention mechanisms. 
  </p>
  <p>
    To improve performance, a scaled dot-product attention mechanism was added. Attention allows the model to prioritize important time steps in the input sequence, helping it respond better to dynamic changes.
  </p>

  <figure>
    <img src="Enc-Dec_Structure.PNG" alt="Baseline Model Block Diagram" />
    <figcaption>Block diagram of the baseline Seq2Seq model.</figcaption>
  </figure>
  <figure>
    <img src="Enc-Dec_Structure_attention.PNG" alt="Model Block with Attention Diagram" />
    <figcaption>Block diagram of the attention-enhanced Seq2Seq model.</figcaption>
  </figure>

  <p>The following equations describe the modelâ€™s hidden state update and output:</p>
  $$
  h_t = \tanh(W_{in} \cdot h_{t-1} + U \cdot x_t + b)
  $$
  $$
  y_t = W_{out} \cdot h_t + c
  $$

  <p>
    The attention mechanism uses alignment scores between encoder and decoder to create a context vector, which iteratively helps the decoder make more accurate predictions.
  </p>

  <h2>Training Predictions Over Time</h2>
  <p>
    To evaluate learning progression, we compared predicted vs. actual control signals at selected training epochs: 2, 5, 10, and 20. The baseline model exhibited irregular fluctuations early in training, while the attention-based model demonstrated smoother alignment with target signals, particularly by epoch 20.
  </p>

  <figure>
    <img src="Baseline_Epoch-loss.PNG" alt="Baseline Predictions Over Time" />
    <figcaption>Prediction outputs from the baseline model at epochs 2, 5, 10, and 20. Notice the instability in early stages and lack of clear tracking.</figcaption>
  </figure>

  <figure>
    <img src="Att_Epoch-loss.PNG" alt="Attention Model Predictions Over Time" />
    <figcaption>Prediction outputs from the attention-based model. Smoother responses and closer alignment are visible, especially by epoch 20.</figcaption>
  </figure>

  <h2>Test Set Performance</h2>
  <p>
    After training, both models were evaluated on a held-out test set of 1,080 samples. The goal was to assess generalization and predictive accuracy under unseen conditions.
  </p>
  <ul>
    <li><strong>Baseline Model Test MSE:</strong> 0.1903</li>
    <li><strong>Attention-Based Model Test MSE:</strong> 0.0914</li>
  </ul>
  <p>
    The attention-based model achieved nearly half the test loss of the baseline model. It responded more effectively to rapid changes in the target control signal and tracked transitions with greater stability.
  </p>

  <figure>
    <img src="Basline-results.PNG" alt="Baseline Test Predictions" />
    <figcaption>Test predictions for the baseline model. Notice drift and lag, particularly at key transition points.</figcaption>
  </figure>

  <figure>
    <img src="Att-results.PNG" alt="Attention Test Predictions" />
    <figcaption>Test predictions for the attention-based model. The prediction reacts at the appropriate changes in temperature.</figcaption>
  </figure>

  <h2>Conclusion</h2>
  <p>
    This project explored machine learning-based temperature control using sequence-to-sequence models with and without attention. The baseline model offered modest performance but struggled with rapid changes. In contrast, the attention-based model showed faster learning, better prediction stability, and significantly reduced test error. The attention mechanism's ability to prioritize key moments in the input sequence proved beneficial in handling dynamic system behavior. These results suggest that attention-based neural models hold promise for future applications in adaptive, data-driven control systems.
  </p>

</body>
</html>
