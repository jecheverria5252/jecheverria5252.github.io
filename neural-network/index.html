<!-- mpc/index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Seq2Seq with Attention – MPC-Inspired Control</title>
  <style>
    body {
      font-family: sans-serif;
      padding: 20px;
      max-width: 900px;
      margin: auto;
      line-height: 1.6;
    }
    img {
      width: 100%;
      height: auto;
      border-radius: 6px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    figure {
      margin: 30px 0;
    }
    figcaption {
      text-align: center;
      font-size: 0.9em;
      color: #555;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <h1>Model Predictive Control with Seq2Seq + Attention</h1>

  <p>This project explores how sequence-based machine learning models can be used to regulate a control system with changing set-points. A baseline Seq2Seq model and an attention-enhanced variant were trained to learn the dynamics of a temperature regulation system, with the goal of predicting control signals that follow desired set-points.</p>

  <h2>Model Overview</h2>
  <p>The architecture is based on an encoder-decoder (Seq2Seq) neural network, commonly used in time series forecasting. Attention was added to improve performance under rapid transitions. The network equations were:</p>

  $$
  h_t = \tanh(W_{in} \cdot h_{t-1} + U \cdot x_t + b)
  $$
  $$
  y_t = W_{out} \cdot h_t + c
  $$

  <h2>Training Results</h2>
  <p>The baseline model struggled with dynamic transitions. By contrast, the attention model better followed changing set-points and adapted to fast shifts.</p>

  <figure>
    <img src="baseline-loss.png" alt="Baseline Training Loss" />
    <figcaption>Loss per batch across epochs for baseline Seq2Seq model</figcaption>
  </figure>

  <figure>
    <img src="attention-loss.png" alt="Attention Model Training Loss" />
    <figcaption>Loss per batch across epochs for attention-based model</figcaption>
  </figure>

  <h2>Learning Progress Over Epochs</h2>
  <figure>
    <img src="baseline-training.png" alt="Baseline Training Progress" />
    <figcaption>Predictions from the baseline model over epochs. The model struggles to learn sharp changes.</figcaption>
  </figure>

  <figure>
    <img src="attention-training.png" alt="Attention Training Progress" />
    <figcaption>Predictions with attention model improve over time. Better alignment with system behavior emerges by Epoch 20.</figcaption>
  </figure>

  <h2>Test Predictions</h2>
  <figure>
    <img src="baseline-test.png" alt="Baseline Test Predictions" />
    <figcaption>Baseline model test output. Fails to respond to shifts in set-point.</figcaption>
  </figure>

  <figure>
    <img src="attention-test.png" alt="Attention Model Test Predictions" />
    <figcaption>Attention-enhanced model tracks the set-point more effectively, especially during transitions.</figcaption>
  </figure>

  <h2>Conclusion</h2>
  <p>Both models were trained to map from past temperature states and future set-points to control signals. While the baseline Seq2Seq model learned the general behavior, it could not handle rapid changes. The attention-based model learned to prioritize key time steps and adjust more effectively. This improvement was confirmed by the lower average mean squared error (MSE) on the test set—nearly 50% lower than the baseline.</p>

  <p>With refinement, attention-based models may support adaptive control tasks where traditional methods are difficult to tune or lack flexibility.</p>
</body>
</html>
