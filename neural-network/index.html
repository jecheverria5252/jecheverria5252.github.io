<!-- mpc/index.html -->
<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Seq2Seq with Attention – MPC-Inspired Control</title>
  <style>
    body {
      font-family: sans-serif;
      padding: 20px;
      max-width: 900px;
      margin: auto;
      line-height: 1.6;
    }
    img {
      width: 100%;
      height: auto;
      border-radius: 6px;
      box-shadow: 0 2px 8px rgba(0, 0, 0, 0.1);
    }
    figure {
      margin: 30px 0;
    }
    figcaption {
      text-align: center;
      font-size: 0.9em;
      color: #555;
    }
  </style>
  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</head>
<body>
  <h1>Temperature Control Using Seq2Seq and Attention</h1>

  <h2>Project Overview</h2>
  <p>
    Closed-loop feedback controllers are commonly used for temperature regulation in the field of process control. Designed for stability and reliability, feedback controllers maintain process stability with minimal deviation from target values. However, feedback regulators face limitations in complex scenarios, as they rely solely on real-time data without learning from past historical information.
  </p>
  <p>
    Recent research on Sequence-to-Sequence machine learning models with attention mechanisms offers promising solutions for more adaptive, dynamic temperature control. The work presented in this thesis project develops a tailored machine learning model based on scaled dot-product attention to predict temperature control signals from error values, using simulated temperature control data. The performance of an attention-based model was compared to a baseline sequence-to-sequence model, demonstrating improved alignment with target control signal and highlighting its potential for effective temperature regulation.
  </p>

  <h2>Background: Closed-Loop Control Systems</h2>
  <p>
    In traditional closed-loop control systems (CLCS), a controller adjusts the input to a system based on the difference between the desired output (the set-point) and the actual output. This is often done using a proportional-integral (PI) or proportional-integral-derivative (PID) controller.
  </p>
  <p>
    PI control combines immediate error correction with a memory of past errors. While simple and effective in many industrial settings, these controllers can struggle when set-points change rapidly or when system dynamics are nonlinear or poorly understood. Derivative terms, often used in PID control, are sensitive to noise and are commonly excluded in temperature regulation.
  </p>

  <figure>
    <img src="Temp-SP_Vs_Control-Signal.png" alt="Set-point vs Control Signal" />
    <figcaption>Figure: Simulated response of a traditional PI-controlled system showing how the control signal responds to temperature changes.</figcaption>
  </figure>

  
  <h2>Models Overview</h2>
  <p>
    Two models were developed for comparison:
  </p>
  <ul>
    <li><strong>Baseline Model:</strong> A basic Seq2Seq model using gated recurrent units (GRUs).</li>
    <li><strong>Attention Model:</strong> An enhanced version with scaled dot-product attention between encoder and decoder.</li>
  </ul>

  <p>
    Both models take as input a sequence of past error, and output a predicted sequence of control signals. The attention model is designed to selectively focus on the most relevant parts of the input sequence (error) when making predictions.
  </p>
    <h2>Attention-Enhanced Model</h2>
  <p>
    Sequence-to-sequence (Seq2Seq) models are commonly used for making predictions over time. In this project, the model takes a sequence of error signal and outputs a predicted sequence of control signals. A baseline model using gated recurrent units was used was used a baseline model.
  </p>
  <p>
    To improve performance, a scaled dot-product attention mechanism was added. Attention allows the model to prioritize important time steps in the input sequence, helping it respond better to dynamic changes.
  </p>
  <figure>
    <img src="Enc-Dec_Structure.png" alt="Baseline Model Block Diagram" />
    <figcaption>Block diagram of the Baseline Seq2Seq model show simplistic.</figcaption>
  </figure>
    <figure>
    <img src="Enc-Dec_Structure_attention.png" alt="Model Block with Attention Diagram" />
    <figcaption>The attention Seq2Seq model block diagram is shown to be simplistic.</figcaption>
  </figure>
  <p>
    The following equations describe the model’s hidden state update and output:
  </p>
  $$
  h_t = \tanh(W_{in} \cdot h_{t-1} + U \cdot x_t + b)
  $$
  $$
  y_t = W_{out} \cdot h_t + c
  $$
  <p>
    The attention mechanism uses alignment scores between encoder and decoder to create a context vector, which helps the decoder make more accurate predictions.
  </p>
</body>
</html>
